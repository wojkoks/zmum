\documentclass[10pt,letterpaper]{article}

\usepackage{polski}		
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{placeins}
\PassOptionsToPackage{hyphens}{url}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\lstloadlanguages{TeX}
\lstset{
	literate={ą}{{\k{a}}}1
           {ć}{{\'c}}1
           {ę}{{\k{e}}}1
           {ó}{{\'o}}1
           {ń}{{\'n}}1
           {ł}{{\l{}}}1
           {ś}{{\'s}}1
           {ź}{{\'z}}1
           {ż}{{\.z}}1
           {Ą}{{\k{A}}}1
           {Ć}{{\'C}}1
           {Ę}{{\k{E}}}1
           {Ó}{{\'O}}1
           {Ń}{{\'N}}1
           {Ł}{{\L{}}}1
           {Ś}{{\'S}}1
           {Ź}{{\'Z}}1
           {Ż}{{\.Z}}1,
	basicstyle=\footnotesize\ttfamily,
}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Report one}

\author{First Author\\
{\tt\small Wojciech Kuś}
\and
Second Author\\
{\tt\small Aleksandra Orzech}
}
\maketitle
%\thispagestyle{empty}
\section{Abstract}

Abstrakcja.

\section{Wprowadzenie}
\section{Zbiór danych}

Analizowany w tym projekcie zbiór danych to \href{https://www.kaggle.com/datasets/atharvaingle/crop-recommendation-dataset}{Crop Recommendation Dataset}, publicznie dostępny na platformie Kaggle. Został on stworzony w celu wspierania budowy modeli uczenia maszynowego do rekomendacji upraw w rolnictwie precyzyjnym.

Zbiór składa się z 2200 rekordów (obserwacji) i 8 kolumn. Każdy rekord reprezentuje pomiary warunków dla konkretnej działki rolnej. Kolumny te dzielą się na:\\[1em]

7 cech wejściowych (atrybutów):
    \begin{itemize}
        \item N: Zawartość azotu w glebie (w mg/kg).
        \item Zawartość fosforu w glebie (w mg/kg).
        \item K: Zawartość potasu w glebie (w mg/kg).
        \item temperature: Średnia temperatura otoczenia (w °C).
        \item humidity: Średnia wilgotność względna powietrza (w \%).
        \item ph: Wartość pH gleby.
        \item rainfall: Suma opadów (w mm).\\[2em]
    \end{itemize}

    1 zmienną docelową (etykietę):
    \begin{itemize}
        \item label: Zmienna określająca typ uprawy, która była prowadzona w danych warunkach. Zbiór zawiera 22 unikalne klasy (m.in. ryż, kukurydza, juta, bawełna, kokos, papaja, pomarańcza, jabłko itd.).
    \end{itemize}

Wszystkie cechy wejściowe są numeryczne (typu float lub int). Oryginalny zbiór danych jest kompletny i nie zawiera żadnych brakujących wartości. Jak wskazano w przeglądzie literatury, problem braków danych został zasymulowany w sposób kontrolowany na potrzeby przeprowadzenia eksperymentów porównawczych.

\section{Przegląd literatury}

Współczesne rolnictwo w coraz większym stopniu opiera się na danych pomiarowych oraz na systemach informatycznych. Takie zjawisko określa się mianem rolnictwa precyzyjnego, którego zadaniem jest wspomaganie podejmowania decyzji biznesowych względem warunków glebowych i klimatycznych na poziomie pól lub ich fragmentów. W efekcie rośnie zapotrzebowanie na metody, które na podstawie dostępnych danych takich jak klimat, żyzność i skład gleby, poziom wilgoci w powietrzu, potrafią wesprzeć właściciela gospodarstwa w wyborze uprawy, zapewnianiu jej optymalnych warunków do rozwoju oraz minimalizacji negatywnego wpływu na środowisko \cite{10.3389/fagro.2025.1566201}.
Jednocześnie prognozy działu ds. Wyżywienia i Rolnictwa ONZ - FAO wskazują, że do 2050 zapotrzebowanie na jedzenie na świecie wzrośnie wraz ze wzrostem liczby ludności, co wymaga zwiększenia światowej produkcji rolnej o około 70\%\cite{FAO2009Feeding} w stosunku do pierwszej dekady lat 2000. W konsekwencji znaczenie rolnictwa precyzyjnego i zapotrzebowania na takie rozwiązania rośnie.

Jednym z takich rozwiązań są systemy rekomendacji upraw (ang. crop recommendation systems) wykorzystujące uczenie maszynowe. Ich zadaniem jest dobranie takiego gatunku rośliny, który jest najlepiej dostosowany do warunków które posiada rolnik i która najprawdopodobniej przyniesie mu największy zysk. Za kulisami takiego systemu zwykle stoi model uczenia maszynowego przyjmujący na wejściu takie dane jak zawartość składników mineralnych w glebie (azot, fosfor, potas), ph gleby, średnią temperaturę powietrza czy sumę opadów i na ich podstawie określa która z upraw jest w danym scenariuszu najlepsza. 


W rozwijaniu takich systemów wykorzystuje się ogólnodostępne zbiory danych, a przykładem takiego zbioru jest Crop Recommendation Dataset. Zbiór ten zawieraja pomiary siedmiu cech opisujących warunki glebowe i klimatyczne: zawartość azotu, fosforu, potasu w glebie, pH gleby, temperatura i wilgotność powietrza oraz opady deszczu. Dla każdego takiego zestawu przypisana jest roślina uprawna która akurat była uprawiana w takich warunkach np. ryż. Zbiór ten jest często wykorzystywany jako punkt odniesienia do porówniania róznych algorytmów klasyfikacyjnych \cite{info16080632, Shastri2025CropRecommendation}.

Istotnym elementem wstępnego przetwarzania danych w zadaniach klasyfikacyjnych jest skalowanie cech. W przypadku takich algorytmów jak k-najbliższych sąsiadów, maszyny wektorów nośnych czy modele liniowe różnice w skali zmiennych mogą prowadzić do dominacji części cech i pogorszenia jakości modelu. Z tego względu w wielu pracach dotyczących systemów rekomendacji upraw stosuje się normalizację lub standaryzację zmiennych wejściowych. Najczęściej wykorzystywane są dwie grupy metod: skalowanie typu Min-Max, które przekształca wartości cechy do zadanego przedziału (np. [0, 1]), oraz standaryzacja, polegająca na odjęciu średniej i podzieleniu przez odchylenie standardowe. Dobór odpowiedniego sposobu skalowania może wpływać na stabilność procesu uczenia i jakość ostatecznych predykcji, zwłaszcza w połączeniu z różnymi strategiami uzupełniania braków danych \cite{DEAMORIM2023109924}.

Agregacja takich danych może być problematyczna i dane rzadko są kompletne. Pomiary właściwości gleby są kosztowne i czasochłonne, a czujniki, którymi zbierane są inne pomiary, mogą okresowo zawodzić. Skutkiem tego są brakujące miejscami dane glebowe jak i klimatyczne. W literaturze opisano różne podejścia do uzupełniania brakujących wartości. Do najprostszych należą metody oparte na zastępowaniu braków średnią, medianą lub inną stałą wartością. Bardziej zaawansowane techniki bazują na modelach regresyjnych, metodach k-najbliższych sąsiadów czy lasach losowych. W obszarze danych glebowych i środowiskowych analizowano m.in. wykorzystanie regresji oraz modeli opartych na uczeniu maszynowym do imputacji braków w pomiarach właściwości gleby. Wyniki tych badań sugerują, że odpowiednio dobrana metoda imputacji może znacząco poprawić jakość późniejszych analiz, w tym budowanych modeli predykcyjnych.

Podsumowując, literatura wskazuje na rosnące znaczenie systemów rekomendacji upraw opartych na uczeniu maszynowym oraz na istotną rolę jakości danych wejściowych w tego typu rozwiązaniach\cite{Alwateer2024MissingData}. W wielu pracach zaproponowano różne algorytmy klasyfikacyjne oraz zestawy cech opisujących warunki glebowo-klimatyczne, jednak problem brakujących danych jest zazwyczaj traktowany w sposób uproszczony lub pomijany. Jednocześnie liczne badania nad imputacją danych glebowych i środowiskowych pokazują, że zastosowanie bardziej zaawansowanych metod uzupełniania braków, takich jak modele regresyjne czy metody oparte na uczeniu maszynowym, może znacząco poprawić jakość późniejszych analiz. Mniejszą uwagę poświęcono natomiast systematycznemu badaniu wpływu różnych metod imputacji oraz skalowania cech na jakość rekomendacji upraw w typowych zbiorach danych, takich jak Crop Recommendation Dataset. Niniejszy projekt wpisuje się w tę lukę badawczą. Jego celem jest porównanie kilku metod uzupełniania braków danych (prostych i bardziej zaawansowanych) oraz dwóch sposobów skalowania cech w zadaniu rekomendacji upraw na podstawie zbioru Crop Recommendation Dataset. Analizie poddano osiem wariantów zbioru danych, różniących się sposobem imputacji i skalowania, co pozwala ocenić wpływ tych decyzji przetwarzania wstępnego na jakość budowanych modeli klasyfikacyjnych.

\section{Motivation}

Rolnictwo przeżywa obecnie dynamiczną rewolucję technologiczną i jest coraz częściej wzbogacany o nowinki takie jak drony monitorujące stan pól z powietrza, autonomiczne ciągniki, zaawansowane sensory IoT (Internetu Rzeczy) zbierające dane w czasie rzeczywistym, czy nawet precyzyjne systemy laserowe do selektywnego zwalczania szkodników i chwastów.

% ... tekst przed pierwszą figurą ...

\begin{figure}[H] % <--- ZMIANA TUTAJ
    \centering
    \includegraphics[width=0.75\linewidth]{21209.jpeg}
    \caption{Globalny stan bezpieczeństwa żywnościowego - ogół}
    \label{fig:placeholder}
\end{figure}

Czerwony/pomarańczowy oznacza, że ogólny system żywnościowy w kraju jest słaby (np. jedzenie jest za drogie, słabej jakości lub łańcuchy dostaw zawodzą), nawet jeśli kraj jest producentem.

\begin{figure}[H] % <--- I ZMIANA TUTAJ
    \centering
    \includegraphics[width=\linewidth]{rp8q0gy73ma91.jpg}
    \caption{Globalny stan bezpieczeństwa żywnościowego - dane dot. importu roślin/zbóż}
    \label{fig:placeholder2}
\end{figure}

Czerwony/pomarańczowy: Kraje, które są krytycznie zależne od importu, aby wyżywić swoją populację (importują ponad 50\% kalorii).

Zielony: Kraje, które są samowystarczalne lub są eksporterami netto żywności (produkują więcej niż konsumują).\\[1em]


Jak wskazują twarde dane, choćby z raportu Global Food Security Index 2022, na świecie istnieją drastyczne dysproporcje w dostępie do żywności. Załączona mapa (Rysunek 1) wyraźnie pokazuje, że podczas gdy Ameryka Północna i Europa Zachodnia cieszą się bardzo wysokim poziomem bezpieczeństwa (indeksy powyżej 80), rozległe obszary Afryki i Azji Południowej znajdują się w krytycznej sytuacji (indeksy często poniżej 50).

Odpowiedzią na te wyzwania jest właśnie rolnictwo precyzyjne - podejmowanie decyzji opartych na danych staje się kluczowe. Systemy rekomendacji upraw, oparte na analizie danych glebowych i klimatycznych, są fundamentalnym elementem transformacji. Pozwalają one rolnikom na podejmowanie świadomych decyzji, co może prowadzić do:

    \begin{itemize}
        \item Zwiększenia plonów poprzez optymalny dobór upraw do lokalnych warunków (kluczowe dla regionów o niskim indeksie bezpieczeństwa).
        \item Redukcji kosztów przez unikanie nadmiernego lub nieodpowiedniego nawożenia.
        \item Minimalizacji negatywnego wpływu na środowisko naturalne.
    \end{itemize}



\section{Evaluation}

Jakiego oczekujecie Państwo wyniku. Jaki wynik będzie z punktu Państwa widzenia satysfakcjonujący i dlaczego? Czy wynik jaki Państwo otrzymacie pozwoli na użycie systemu po wdrożeniu?


Oczekujemy, że nasz eksperyment pozwoli na sformułowanie jasnych wniosków dotyczących tego, która kombinacja metod imputacji i skalowania cech jest najbardziej efektywna dla zadania rekomendacji upraw na badanym zbiorze danych.

Za satysfakcjonujący wynik uznamy wykazanie statystycznie istotnych różnic w metrykach oceny modeli (takich jak dokładność, precyzja, czy pełność) w zależności od zastosowanego potoku przetwarzania wstępnego.

\section{Zasoby}

Jakich zasobów będą Państwo używać, w czym będzie projekt zaimplementowany.

Projekt jest zaimplementowany w języku Python oraz prowadzony w środowisku Google Colab. Biblioteki wykrozystane w projekcie:
\begin{itemize}
    \item Pandas
    \item NumPy
    \item Scikit-learn (sklearn)
    \item Matplotlib / Seaborn
\end{itemize}
\section{Zastosowane metody}

W ramach projektu porównane zostaną różne strategie przetwarzania wstępnego, obejmujące kombinacje metod imputacji oraz skalowania, a następnie oceniony zostanie ich wpływ na wybrane algorytmy klasyfikacyjne.

\subsection{Metody uzupełniania braków danych (Imputacja)}

Jak wspomniano w przeglądzie literatury, porównane zostaną metody proste i zaawansowane.

\subsubsection{Metody proste (jednozmiennowe)}
\begin{itemize}
    \item \textbf{Średnia (\texttt{SimpleImputer(strategy='mean')}):} Zastąpienie brakujących wartości średnią arytmetyczną z danej kolumny.
    \item \textbf{Mediana (\texttt{SimpleImputer(strategy='median')}):} Zastąpienie brakujących wartości medianą z danej kolumny (metoda bardziej odporna na wartości odstające).
\end{itemize}

\subsubsection{Metody zaawansowane (wielozmiennowe)}
\begin{itemize}
    \item \textbf{k-Najbliższych Sąsiadów (\texttt{KNNImputer}):} Uzupełnienie braków na podstawie średniej wartości od k-najbliższych sąsiadów (rekordów) w przestrzeni cech.
    \item \textbf{Modele regresyjne (\texttt{IterativeImputer}):} Traktowanie każdej cechy z brakami jako zmiennej zależnej i budowanie modelu regresyjnego (np. lasu losowego lub regresji bayesowskiej) na podstawie pozostałych cech, aby przewidzieć brakujące wartości.
\end{itemize}

\subsection{Metody skalowania cech}

Przetestowane zostaną dwie najpopularniejsze techniki skalowania.

\begin{itemize}
    \item \textbf{Standaryzacja (\texttt{StandardScaler}):} Przeskalowanie cech tak, aby miały średnią równą $0$ i odchylenie standardowe równe $1$. Jest to preferowana metoda dla algorytmów zakładających normalny rozkład danych.
    \item \textbf{Normalizacja Min-Max (\texttt{MinMaxScaler}):} Przeskalowanie cech do zadanego przedziału, najczęściej $[0, 1]$. Metoda ta jest użyteczna, gdy algorytm (np. k-NN) wymaga cech w ściśle określonym zakresie.
\end{itemize}

\subsection{Modele klasyfikacyjne}

Do oceny potoków przetwarzania wykorzystamy zestaw algorytmów klasyfikacyjnych o różnej charakterystyce i wrażliwości na wstępne przetwarzanie:

\begin{itemize}
    \item \textbf{k-Najbliższych Sąsiadów (k-NN):} Model oparty na odległości, bardzo wrażliwy na skalowanie cech.
    \item \textbf{Maszyna Wektorów Nośnych (SVM):} Model również wrażliwy na skalowanie, szukający optymalnej hiperpłaszczyzny separującej klasy.
    \item \textbf{Lasy Losowe (Random Forest):} Model zespołowy oparty na drzewach decyzyjnych, z natury odporny na różnice w skalach cech. Posłuży jako model bazowy do oceny wpływu samego skalowania.
    \item \textbf{Regresja Logistyczna:} Model liniowy, który zazwyczaj zyskuje na standaryzacji cech.
\end{itemize}

\section{Eksperyment}
\subsection{Preprocessing}
W ramach preprocessingu dancyh wykonano serię kroków mających na celu przygotowanie i optymalizację danych do dalszej analizy oraz budowy modeli klasyfikacyjnych. Dane wczytano do obiektu DataFrame za pomocą \textit{pandas}. Umożliwiło to wykorzystanie API pandas w celu wstępnej interpretacji i przetwarania danych. Za pomocą df.describe() oraz df.isna() wykazano, że zbiór domyślnie nie zawiera żadnych brakujących danych, co defacto jest nietypowe w kontekście danych rolniczych czy przemysłowych, gdzie taka niekompletność to zjawisko powszechne.


\begin{table}[h!]
    \centering
    \caption{Statystyki opisowe zbioru danych rolniczych obejmujące makroelementy gleby (N, P, K), parametry środowiskowe (temperatura, wilgotność, opad) oraz odczyn pH.}
    \label{tab:statystyki}
    \begin{tabular}{lrrrrrrr}
        \toprule
         & N & P & K & temperature & humidity & ph & rainfall \\
        \midrule
        count & 2200.00 & 2200.00 & 2200.00 & 2200.00 & 2200.00 & 2200.00 & 2200.00 \\
        mean & 50.55 & 53.36 & 48.15 & 25.62 & 71.48 & 6.47 & 103.46 \\
        std & 36.92 & 32.99 & 50.65 & 5.06 & 22.26 & 0.77 & 54.96 \\
        min & 0.00 & 5.00 & 5.00 & 8.83 & 14.26 & 3.50 & 20.21 \\
        25\% & 21.00 & 28.00 & 20.00 & 22.77 & 60.26 & 5.97 & 64.55 \\
        50\% & 37.00 & 51.00 & 32.00 & 25.60 & 80.47 & 6.43 & 94.87 \\
        75\% & 84.25 & 68.00 & 49.00 & 28.56 & 89.95 & 6.92 & 124.27 \\
        max & 140.00 & 145.00 & 205.00 & 43.68 & 99.98 & 9.94 & 298.56 \\
        \bottomrule
    \end{tabular}
\end{table}




Jako, że jednym z wymogów projektu było wykorzystanie zbioru danych zawierającego conajmniej 10\% brakujących danych, zdecydowano się na kontrolowaną modyfikację datasetu wprowadzającej takie braki.
W tym celu zaimplementowano funkcję \textit{add\_missing\_values}
\begin{verbatim}
def add_missing_values(df: pd.DataFrame, 
    column_name: str, 
    missing_frac=0.1, 
    random_state=None):
np.random.seed(random_state)

if column_name not in df.columns:
    raise ValueError(f"Column '{column_name}' 
        not found in DataFrame. Available columns: {list(df.columns)}")

df_copy = df.copy()
indices = df_copy.sample(frac=missing_frac, random_state=random_state)
    .index
df_copy.loc[indices, column_name] = np.nan

return df_copy
\end{verbatim}

Następnie wykorzystano ją do modyfikacji 3 kolumn: ph, temperature, humidity:
\begin{verbatim}
df = add_missing_values(df, 'ph', random_state=10)
df = add_missing_values(df, 
    'temperature', 
    missing_frac=0.10, 
    random_state=10)
df = add_missing_values(df, 'humidity', missing_frac=0.17, random_state=10)
\end{verbatim}
W efekcie otrzymano zbiór danych z częściowo pustymi wierszami, co umozliwiło dalsze testowanie metod imputacji braków danych w warunkach zbliżonych do rzeczywistych,

\begin{table}[h!]
    \centering
    \caption{Statystyki opisowe zbioru danych po wprowadzeniu kontrolowanych braków w kolumnach \textit{ph}, \textit{temperature} oraz \textit{humidity}.}
    \label{tab:statystyki_missing}
    \begin{tabular}{lrrrrrrr}
        \toprule
         & N & P & K & temperature & humidity & ph & rainfall \\
        \midrule
        count & 2200.00 & 2200.00 & 2200.00 & 1980.00 & 1826.00 & 1980.00 & 2200.00 \\
        mean & 50.55 & 53.36 & 48.15 & 25.61 & 71.53 & 6.47 & 103.46 \\
        std & 36.92 & 32.99 & 50.65 & 5.08 & 22.38 & 0.78 & 54.96 \\
        min & 0.00 & 5.00 & 5.00 & 9.47 & 14.32 & 3.51 & 20.21 \\
        25\% & 21.00 & 28.00 & 20.00 & 22.78 & 60.14 & 5.97 & 64.55 \\
        50\% & 37.00 & 51.00 & 32.00 & 25.61 & 80.68 & 6.43 & 94.87 \\
        75\% & 84.25 & 68.00 & 49.00 & 28.57 & 90.02 & 6.92 & 124.27 \\
        max & 140.00 & 145.00 & 205.00 & 43.68 & 99.98 & 9.94 & 298.56 \\
        \bottomrule
    \end{tabular}
\end{table}

Po wprowadzeniu niekompletności danych konieczne było przeprowadzenie procesu imputacji co jest jednym z wymagań projektu. W przypadku cech \textit{ph} oraz \textit{humidity} zastosowano klasyczne imputacje z wykorzystaniem metod agregacyjnych \textit{pandas}, takich jak średnia i mediana, co pozwoliło w prosty sposób uzupełnić brakujące wartości przy minimalnym zaburzeniu rozkładu zmiennych. W celu dokładniejszego odtworzenia wartości temperatury wykorzystano bardziej zaawansowaną metodę predykcyjną opartą na regresji. Zastosowano model \textit{RandomForestRegressor}, który trenowano na pełnych obserwacjach, a następnie użyto do przewidywania brakujących wartości temperatury. Model osiągnął błąd średniokwadratowy na poziomie około 2.69~$^\circ$C, co potwierdziło jego zdolność do wiarygodnej rekonstrukcji brakujących obserwacji. Uzyskane predykcje zastąpiono brakującymi wartościami przy użyciu masek \textit{pandas}, co pozwoliło zachować spójność struktury danych oraz ograniczyć artefakty wynikające z niekompletności zbioru.


W celu wzbogacenia reprezentacji danych oraz uchwycenia jakichś dodatkowych zależności istotnych z punktu widzenia wzrostu roślin przeprowadzono proces ekstrakcji cech. W pierwszej kolejności skonstruowano wskaźniki oparte na stosunkach makroelementów, takie jak N/P czy N/K. Proporcje te są kluczowe, ponieważ rośliny wymagają określonego zbilansowania składników odżywczych - na przykład nadmierna ilość azotu w stosunku do fosforu sprzyja szybkiemu rozwojowi liści kosztem owocowania.
\begin{verbatim}
df['N_P_ratio'] = df['N'] / (df['P'] + 0.00001)
\end{verbatim}


Następnie utworzono indeksy łączące warunki środowiskowe, m.in. wskaźnik stosunku temperatury do wilgotności, dla przykładu wysokie temperatury oraz wysoka wilgotność mogą wskazywać na klimat równikowy. Parametr ten może mieć istotny wpływ na ryzyko występowania chorób roślin, zwłaszcza grzybowych. Dodatkowo skonstruowano syntetyczne wskaźniki żyzności gleby, bazujące na sumarycznym poziomie głównych makroelementów. Zmienna taka może być szczególnie użyteczna dla upraw wymagających ogólnie zasobnego podłoża, niezależnie od dokładnych proporcji poszczególnych pierwiastków.
\begin{verbatim}
df['temp_humidity_index'] = df['temperature'] * df['humidity'] / 100
df['total_nutrients'] = df['N'] + df['P'] + df['K']
\end{verbatim}

Kolejnym etapem preprocessingu było przeskalowanie zmiennych numerycznych w celu ujednolicenia ich zakresów. 
W tym celu zastosowano metodę \textit{StandardScaler} z biblioteki \textit{scikit-learn}, która standaryzuje każdą cechę według zależności:
\[
x' = \frac{x - \mu}{\sigma},
\]
gdzie $\mu$ oznacza średnią, a $\sigma$ odchylenie standardowe danej kolumny. 
Operacji skalowania poddano wszystkie cechy numeryczne oprócz etykiety klasowej.
\begin{verbatim}
from sklearn.preprocessing import StandardScaler

cols_to_scale = df.columns.drop('label')
scaler = StandardScaler()

df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])
\end{verbatim}
\subsection{Eksploracyjna analiza danych}
Eksploracyjna analiza danych to kluczowy etap poznawania zbioru danych. Jej celem jest zrozumienie struktury, identyfikacja zależności między cechami, wykrycie wartości odstająctch oraz ocena jakości danych. Modele uczenia maszynowego są wrażliwe na takie cechy co może negatywnie wpłynąć na końcową dokładność i jakość modelu.
\begin{figure}[H]
\begin{center}
   \includegraphics[width=1\linewidth]{figures/boxplots.png}
\end{center}
   \caption{Wykresy pudełkowe dla cech numerycznych.}
\label{fig:other-figure}
\end{figure}
Wszystkie cechy zawierają wartości odstające co jest zgodne z charakterystyką danych klimatycznych i glebowych. Rozkłady te uzasadniają użycie \textit{StandardScalera} w celu skalowania cech, co minimalizuje dominację cech o dużej skali.


\begin{figure}[H]
\begin{center}
   \includegraphics[width=1\linewidth]{figures/pairplots.png}
\end{center}
   \caption{Pairplot przedstawiający zależności między cechami oraz ich rozkłady jednowymiarowe dla Crop Recommendation.}
\label{fig:other-figure}
\end{figure}
Pairplot przedstawia rozkłady poszczególnych cech oraz zalożności między zmiennymi po ich standaryzacji. Większość par cech nie wykazuje silnych zależności liniowych. Choć same cechy nie są silnie skorelowane, grupa roślin tworzy zbiory punktów, które modele klasyfikacyjne mogą wykorzystywać do rozróżniania klas.


\begin{figure}[H]
\begin{center}
   \includegraphics[width=1\linewidth]{figures/macierz_pearsona.png}
\end{center}
   \caption{Macierz korelacji Parsona.}
\label{fig:other-figure}
\end{figure}
Z definicji macierz korelacji Pearsona przedstawia siłę i kierunek zależności liniowych pomiędzy wszystkimi parami zmiennych numerycznych w zbiorze danych. Wartości w macierzy mieszczą się w przedziale od -1 do 1 gdzie 1 oznacza pełną dodatnią korelację liniową, -1 pełną ujemną, natomiast 0 oznacza brak liniowej zależności.Najbardziej widoczną zależnością jest wysoka dodatnia korelacja pomiędzy fosforem (P) a potasem (K). Oznacza to, że działki o wyższym poziomie fosforu mają zwykle także wyższy poziom potasu.
Każda zmienna opisuje inną właściwość środowiska i nie wynika bezpośrednio z pozostałych parametrów. To oznacza, że cechy nie są redundantne i każda wnosi odrębną informację.


\begin{figure}[H]
\begin{center}
   \includegraphics[width=1\linewidth]{figures/macierz_spearmana.png}
\end{center}
   \caption{Macierz korelacji Spearmana.}
\label{fig:other-figure}
\end{figure}
Macierz korelacji Spearmana przedstawia siłę i kierunek zależności monotonicznych (a niekoniecznie liniowych) między zmiennymi. Z tej macierzy wynika, że brak jest silnych zależności między cechami. Najsilniejszą zależnością posiada potas oraz wilgoć. Może to wynikać z faktu, że niektóre rośliny lubiące wilgotne środowisko wymagają jednocześnie bardziej zasobnej gleby. \\[1em]


% --- TEKST ANALIZY 1 (Imputacja) ---
\noindent
Wpływ metody imputacji na rozkład danych (widoczny na Rysunku \ref{fig:imputacja_porownanie}):
\begin{itemize}
    \item Metody proste (średnia, mediana) prowadzą do sztucznej redukcji wariancji w danych. Zastąpienie wszystkich braków jedną, stałą wartością powoduje nienaturalne "zwężenie" rozkładu cechy (co było widoczne na wykresach pudełkowych), potencjalnie wprowadzając model w błąd co do rzeczywistej zmienności danych.
    \item Metody predykcyjne (KNN, Random Forest) są znacznie bardziej adekwatne. Poprzez estymację brakujących wartości na podstawie innych cech, zachowują one naturalną dyspersję i strukturę danych, co czyni zbiór bardziej wiarygodnym i reprezentatywnym.
\end{itemize}

% --- Rysunek 1: Porównanie Metod Imputacji ---
% Plik 'Untitled.png' to ten z komórki 7 (porównanie imputacji)
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Untitled.png}
    \caption{Porównanie dystrybucji danych dla cech \texttt{temperature}, \texttt{humidity} oraz \texttt{ph} po zastosowaniu czterech różnych metod imputacji (Mean, Median, KNN, Mixed-RF).}
    \label{fig:imputacja_porownanie}
\end{figure}


% --- TEKST ANALIZY 2 (Skalowanie) ---
\noindent
Analiza wpływu skalowania danych (widoczna na Rysunku \ref{fig:skalowanie_porownanie}):
\begin{itemize}
    \item Dane surowe (przed skalowaniem) charakteryzowały się znaczną dysproporcją w rzędach wielkości (np. cecha \texttt{rainfall} vs \texttt{ph}). Stosowanie algorytmów wrażliwych na dystans (jak KNN, SVM, czy sieci neuronowe) na takich danych prowadziłoby do dominacji cech o większych wartościach liczbowych, niezależnie od ich rzeczywistego znaczenia.
    \item Obie metody skalowania skutecznie rozwiązały ten problem. Normalizacja Min-Max ujednoliciła zakres wszystkich cech do [0, 1], co jest szczególnie pożądane w sieciach neuronowych. Standaryzacja Z-score scentralizowała rozkład każdej cechy wokół zera ($\mu=0$) przy jednostkowej wariancji ($\sigma=1$), co jest uważane za standardowe i najbardziej uniwersalne podejście dla większości algorytmów ML.
\end{itemize}

% --- Rysunek 2: Porównanie Metod Skalowania ---
% Plik 'Untitledzmum.png' to ten z komórki 8 (porównanie skalowania)
% --- Rysunek 2: Porównanie Metod Skalowania ---
\begin{figure}[H]  % <--- ZMIANA: [H] zamiast [h!]
    \centering
    \includegraphics[width=1\linewidth]{Untitledzmum.png}
    \caption{Porównanie wpływu metod skalowania (Original vs. MinMax vs. Standard) na rozkład poszczególnych cech. Analiza na przykładzie zbioru z imputacją metodą mieszaną (Mixed-RF).}
    \label{fig:skalowanie_porownanie}
\end{figure}


\subsection{Trening modelu}
\subsubsection{Dobór algorytmów klasyfikacyjnych}
W celu zapewnienia różnorodności perspektyw decyzyjnych, do eksperymentu wybrano zestaw modeli reprezentujących różne rodziny algorytmów uczenia maszynowego. Trenowano je na zbiorze danych poddanym wcześniejszemu "sabotażowi" (zaszumienie i braki danych), aby wyłonić rozwiązanie najbardziej odporne na trudne warunki. Wykorzystano:
Drzewa Decyzyjne (Decision Tree): Jako model bazowy (baseline), cechujący się wysoką interpretowalnością, ale podatnością na przeuczenie (overfitting).

\begin{itemize}
    \item Algorytmy oparte na sąsiedztwie (KNN): Aby sprawdzić, jak degradacja danych wpływa na modele geometryczne, wrażliwe na odległości między punktami.
    \item Metody Zespołowe: Random Forest – wybrano go ze względu na naturalną zdolność do redukcji wariancji i odporność na szum poprzez uśrednianie wyników wielu drzew. 
    \item Gradient Boosting: Wykorzystano biblioteki XGBoost oraz LightGBM. Są to algorytmy sekwencyjne, które korygują błędy poprzedników. LightGBM wybrano dodatkowo ze względu na jego natywną obsługę braków danych (NaN).
    \item Modele Zespołowe Wyższego Rzędu:
    \begin{itemize}
        \item Voting Classifier (Soft): Głosowanie większościowe oparte na prawdopodobieństwach zwróconych przez Random Forest, XGBoost i KNN.
        \item Stacking Classifier: Architektura dwupoziomowa, gdzie meta-klasyfikator (Regresja Logistyczna) uczy się, jak najlepiej połączyć predykcje modeli bazowych (RF i XGBoost).
    \end{itemize}
\end{itemize}

\begin{verbatim}
models = {
    #modele scikit-learn
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5),

    #mdele zewn
    'XGBoost': XGBClassifier(eval_metric='mlogloss', use_label_encoder=False),
    'LightGBM': LGBMClassifier(verbose=-1),

    #modele zespołowe
    'Voting (Soft)': VotingClassifier(
        estimators=[
            ('dt', DecisionTreeClassifier()),
            ('rf', RandomForestClassifier()),
            ('knn', KNeighborsClassifier())
        ], voting='soft'
    ),
    'Stacking': StackingClassifier(
        estimators=[
            ('rf', RandomForestClassifier(n_estimators=50)),
            ('knn', KNeighborsClassifier())
        ], final_estimator=LogisticRegression()
    )
}
\end{verbatim}

\subsubsection{Procedura Treningowa}
Pierwszy eskperyment przeprowadzono na danych po pierwszym preprocessingu. Ze względu na charakterystykę zbioru danych (nierealistycznie idealne dane), eksperyment musiał zostać powtórzony oraz zaimplementowano funkcję wprowadzającą celowe uszkodzenia do zbioru treningowego.

\begin{itemize}
    \item Szum pomiarowy: Do kolumn N, P, K dodano losowy szum z rozkładu normalnego (\μ=0,\σ=8), symulując niedokładność tanich czujników glebowych.
    \item Awarie czujników: Losowo usunięto wartości w kluczowych kolumnach środowiskowych: ph (30\% braków), humidity (25\% braków) oraz temperature (20\% braków).
\end{itemize}


\begin{verbatim}
def run_heavy_sabotage(dataframe, random_state=42):
    np.random.seed(random_state)
    df_broken = dataframe.copy()
    
    #1.Szum pomiarowy
    for col in ['N', 'P', 'K']:
        noise = np.random.normal(0, 8, size=len(df_broken))
        df_broken[col] = (df_broken[col] + noise).clip(lower=0)
    
    #2.Losowe usuwanie danych
    #pH:30%, wilgotnosc:25%, temperatura:20%
    for col, frac in [('ph', 0.30), ('humidity', 0.25), ('temperature', 0.20)]:
        n_missing = int(len(df_broken) * frac)
        idx = np.random.choice(df_broken.index, n_missing, replace=False)
        df_broken.loc[idx, col] = np.nan

    return df_broken
\end{verbatim}


Proces uczenia podzielono na dwa etapy weryfikacji:

\begin{enumerate}
    \item Ranking Wstępny (Turniej Modeli): Zbiór danych podzielono ręcznie na część treningową (80\%) i testową (20\%) z wykorzystaniem losowego tasowania indeksów (numpy.random.shuffle). Wszystkie modele zostały wytrenowane na tym samym podziale, a ich skuteczność (Accuracy) zmierzona na zbiorze testowym pozwoliła wyłonić model zwycięski ("Winner").
    \item Weryfikacja Krzyżowa (Cross-Validation): Dla zwycięskiego modelu przeprowadzono rygorystyczną, 5-krotną walidację krzyżową (K-Fold Cross-Validation). Procedura ta została zaimplementowana ręcznie (iteracyjne dzielenie zbioru na 5 rozłącznych części), zgodnie z założeniami projektu, aby wyeliminować wpływ losowego doboru zbioru testowego na końcową ocenę i uzyskać wiarygodny estymator błędu generalizacji.
\end{enumerate}



\subsection{Wyniki Eksperymentów}
Przeprowadzono porównanie skuteczności modeli w dwóch scenariuszach.

\subsubsection{Scenariusz "Idealny"}
Na oryginalnym zbiorze danych (bez szumu i braków) modele osiągały bardzo wysoką skuteczność, co sugeruje, że problem klasyfikacji na danych idealnych jest relatywnie łatwy. Dla porównywanych algorytmów uzyskano następujące wartości Accuracy (zbiór testowy):

\begin{enumerate}
    \item Random Forest, XGBoost i LightGBM: \textbf{99.77\%} (0.9977) – na czystych danych modele zespołowe i boosting niemal perfekcyjnie rozdzielają klasy.
    \item Decision Tree oraz Voting (Soft): \textbf{99.32\%} (0.9932) – bardzo dobry wynik, ale niższy niż metody zespołowe/boosting.
    \item Stacking: \textbf{99.09\%} (0.9909) – łączenie modeli daje wysoką jakość, choć w tym wariancie ustępuje najlepszym.
    \item KNN: \textbf{97.27\%} (0.9727) – model oparty na dystansie jest bardziej wrażliwy na charakterystykę cech i granice klas.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/training/ideal_training_result.png}
    \caption{Ranking modeli w scenariuszu idealnym (bez dodatkowego szumu i braków danych): porównanie Accuracy na zbiorze testowym.}
    \label{fig:placeholder}
\end{figure}



\subsubsection{Scenariusz "Rzeczywisty" (po uszkodzeniu danych)}
Po wprowadzeniu szumu i braków danych zaobserwowano spadek wydajności wszystkich modeli. Ranking modeli w trudnych warunkach przedstawia się następująco (wyniki na zbiorze testowym):
\begin{enumerate}
    \item LightGBM: \textbf{96.14\%} (0.9614) oraz XGBoost: \textbf{95.91\%} (0.9591) – najwyższa odporność na szum i braki danych.
    \item Random Forest: \textbf{94.32\%} (0.9432) – odporność na wariancję dzięki uśrednianiu wielu drzew.
    \item Stacking: \textbf{93.86\%} (0.9386) oraz Voting (Soft): \textbf{92.27\%} (0.9227) – metody zespołowe oparte o agregację poprawiają stabilność, ale ustępują boostingowi.
    \item Decision Tree: \textbf{89.55\%} (0.8955) oraz KNN: \textbf{86.59\%} (0.8659) – modele proste/dystansowe najsilniej tracą na degradacji jakości danych.
\end{enumerate}


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/training/real_training_result.png}
    \caption{Ranking wszystkich modeli w scenariuszu "rzeczywistym" (po uszkodzeniu danych): porównanie Accuracy na zbiorze testowym.}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/training/gbm_ver.png}
    \caption{Przykładowa wizualizacja jakości wybranego modelu boostingowego (LightGBM) w scenariuszu z uszkodzonymi danymi.}
    \label{fig:placeholder}
\end{figure}


\subsubsection{Weryfikacja: Ręczna Walidacja Krzyżowa}
Aby potwierdzić, że wynik zwycięskiego modelu nie jest dziełem przypadku, przeprowadzono 5-krotną walidację krzyżową (K-Fold). \\
Odchylenie wyników między foldami było niewielkie, co świadczy o stabilności modelu mimo "zepsutych" danych.\\
W czasie testowania zauważono znaczne pogorszenie modelu RandomForest, tj. nie był już on najskuteczniejszym modelem w porównaniu do wyników przy sytuacji idealnej. Podział danych jest losowy, więc przy ponownym podziale kodu na test i trening, dokładność może się róznić, ale widać tendencję w zaradności modeli. Najgorszej radzi sobie KNN i Decision Tree. Natomiast w czołówce znajdują się XGBoost oraz LightGBM.


\begin{verbatim}
k = 5
cv_scores = []
fold_size = len(X) // k
ind = np.arange(len(X))
np.random.shuffle(ind)

print(f"\nRęczna Walidacja Krzyżowa (5-fold) dla {best_model_name} ---")
for i in range(k):
    val_indices = ind[i*fold_size : (i+1)*fold_size]
    train_indices = np.concatenate([ind[:i*fold_size], ind[(i+1)*fold_size:]])

    winner_model.fit(X[train_indices], y_enc[train_indices])
    preds = winner_model.predict(X[val_indices])
    score = np.mean(preds == y_enc[val_indices])
    cv_scores.append(score)
    print(f"Fold {i+1}: {score:.4f}")

final_cv_score = np.mean(cv_scores)
\end{verbatim}


\subsection{Cz.3: Selekcja cech i optymalizacja modelu}
W kolejnej części pracy wykonano dodatkową optymalizację, której celem było (1) sprawdzenie wpływu doboru cech na jakość klasyfikacji oraz (2) poprawa jakości predykcji poprzez strojenie hiperparametrów.

Zgodnie z wymaganiami projektu optymalizacji poddano wyłącznie najlepszy model z turnieju, tj. \textbf{LightGBM}. Porównania wykonywano w sposób spójny: metryką główną było $F_1$ (macro), raportowane zarówno w \textbf{5-krotnej walidacji krzyżowej} (StratifiedKFold), jak i na wydzielonym zbiorze testowym.

W Cz.3 analizowano model uczony na zbiorze po preprocessingu oraz inżynierii cech. W finalnym wariancie wykorzystywano łącznie \textbf{9 cech}: 7 oryginalnych (N, P, K, temperature, humidity, ph, rainfall) oraz 2 cechy inżynieryjne (\texttt{N\_P\_ratio}, \texttt{total\_nutrients}).

\subsubsection{Selekcja cech}
W pierwszym kroku przeanalizowano zależności pomiędzy cechami (macierz korelacji), a następnie porównano dwa podejścia do selekcji cech:
\begin{itemize}
    \item \textbf{SelectKBest} (mutual information) — wybór $k$ cech o najwyższej zależności z etykietą.
    \item \textbf{Permutation Importance} — ocena spadku jakości po losowej permutacji pojedynczej cechy.
\end{itemize}

Macierz korelacji (Rys. \ref{fig:opt_corr_heatmap}) służyła jako kontrola redundancji. Zauważalne są zależności pomiędzy niektórymi parami cech (np. P i K), a także powiązania między cechami inżynieryjnymi i ich składowymi. Nie zaobserwowano jednak sytuacji, w której jedna cecha w oczywisty sposób w pełni zastępowałaby inną (brak bardzo silnych korelacji w większości par), co sugeruje, że agresywne odrzucanie zmiennych może prowadzić do utraty informacji.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/optimization/correlation_heatmap.png}
    \caption{Macierz korelacji cech wykorzystana jako wstęp do selekcji cech (Cz.3).}
    \label{fig:opt_corr_heatmap}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/optimization/feature_selection_comparison.png}
    \caption{Porównanie jakości ($F_1$ macro) dla metod selekcji cech (Cz.3) w walidacji krzyżowej oraz na zbiorze testowym.}
    \label{fig:opt_feature_selection_comparison}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Porównanie metod selekcji cech - F1 Score (CV i test).}
    \label{tab:feature_selection}
    \begin{tabular}{lrrrr}
        \toprule
        Metoda & Liczba\_cech & F1\_CV & F1\_test & Opis \\
        \midrule
        Baseline (wszystkie cechy) & 9 & 0.9531 & 0.9546 & Model bez selekcji cech \\
        SelectKBest (k=7) & 7 & 0.9520 & 0.9535 & Mutual information \\
        Permutation Importance (top 4) & 4 & 0.9490 & 0.9505 & Najważniejsze cechy \\
        \bottomrule
    \end{tabular}
\end{table}

Wyniki selekcji cech (Rys. \ref{fig:opt_feature_selection_comparison} oraz \ref{fig:opt_feature_selection_table}) pokazują, że najlepszy rezultat uzyskano dla konfiguracji \textbf{bez redukcji cech}. Model bazowy (wszystkie cechy) osiągnął $F_1$\textsubscript{CV}$\approx 0.9531$ oraz $F_1$\textsubscript{test}$\approx 0.9546$. Metoda SelectKBest wskazała jako optymalne $k=9$ (czyli pełen zestaw cech), co w praktyce oznacza brak realnej selekcji i taki sam wynik jak baseline. Z kolei redukcja do 4 cech wybranych przez Permutation Importance spowodowała silny spadek jakości ($F_1$\textsubscript{CV}$\approx 0.8801$, $F_1$\textsubscript{test}$\approx 0.8783$).

Wniosek: w tym zadaniu informacja jest rozproszona pomiędzy cechy, a LightGBM korzysta z \textbf{interakcji} pomiędzy nimi; redukcja wymiaru (zwłaszcza do bardzo małej liczby cech) pogarsza jakość. Do dalszego strojenia hiperparametrów przyjęto zatem \textbf{pełen zestaw 9 cech}.

\subsubsection{Optymalizacja hiperparametrów (Grid Search, Bayesian, AutoML, GA)}
Po wyłonieniu najlepszego modelu bazowego przeprowadzono strojenie hiperparametrów wyłącznie dla zwycięzcy (w celu ograniczenia kosztu obliczeń). Porównano następujące podejścia:
\begin{itemize}
    \item \textbf{GridSearchCV} — przeszukiwanie zdefiniowanej siatki parametrów.
    \item \textbf{Optuna} — optymalizacja bayesowska (TPE) maksymalizująca wynik walidacji krzyżowej.
    \item \textbf{TPOT} — podejście AutoML automatycznie dobierające model/pipeline.
    \item \textbf{DEAP} — algorytm genetyczny strojący hiperparametry.
\end{itemize}

Wszystkie metody (poza TPOT, który sam konstruuje pipeline) były porównywane w tej samej konfiguracji walidacji (5-fold StratifiedKFold) i z tą samą metryką celu ($F_1$ macro). GridSearchCV testował skończoną siatkę parametrów LightGBM, Optuna realizowała przeszukiwanie bayesowskie w przestrzeni hiperparametrów (25 prób), a DEAP stosował prosty algorytm ewolucyjny (populacja 15, 8 generacji). TPOT został użyty jako reprezentant AutoML w celu spełnienia wymagania projektowego; w tym przypadku kluczowe jest rozróżnienie pomiędzy \textit{wynikiem na zbiorze treningowym} a wynikiem z walidacji krzyżowej.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/optimization/hyperparam_methods_comparison.png}
    \caption{Porównanie metod optymalizacji hiperparametrów: $F_1$ macro w walidacji krzyżowej vs $F_1$ macro na teście (Cz.3).}
    \label{fig:opt_hyperparam_methods_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/optimization/hyperparam_methods_table.png}
    \caption{Tabela porównawcza metod optymalizacji hiperparametrów (Cz.3) wraz z wartościami $F_1$ (CV/test) i krótkim opisem kosztu obliczeń.}
    \label{fig:opt_hyperparam_methods_table}
\end{figure}

Na podstawie zestawienia (Rys. \ref{fig:opt_hyperparam_methods_comparison} oraz \ref{fig:opt_hyperparam_methods_table}) widać, że strojenie hiperparametrów poprawia jakość w porównaniu do konfiguracji bazowej. Wyniki dla LightGBM przedstawiają się następująco:
\begin{itemize}
    \item \textbf{Baseline}: $F_1$\textsubscript{CV}$\approx 0.9531$, $F_1$\textsubscript{test}$\approx 0.9546$.
    \item \textbf{GridSearchCV}: $F_1$\textsubscript{CV}$\approx 0.9592$, $F_1$\textsubscript{test}$\approx 0.9589$.
    \item \textbf{Optuna}: $F_1$\textsubscript{CV}$\approx 0.9615$, $F_1$\textsubscript{test}$\approx 0.9632$.
    \item \textbf{DEAP (genetyczny)}: $F_1$\textsubscript{CV}$\approx 0.9641$, $F_1$\textsubscript{test}$\approx 0.9628$.
    \item \textbf{TPOT (AutoML)}: wynik na teście $F_1$\textsubscript{test}$\approx 0.9465$; wartość raportowana jako „CV” w implementacji pochodziła z \texttt{fitted\_pipeline\_.score} (ocena na zbiorze treningowym) i nie jest porównywalna z $F_1$ z walidacji krzyżowej.
\end{itemize}

W praktyce o wyborze metody decyduje nie tylko maksymalizacja $F_1$\textsubscript{CV}, lecz przede wszystkim generalizacja na dane testowe. Najlepszy wynik testowy uzyskano dla \textbf{Optuna} ($F_1$\textsubscript{test}$\approx 0.9632$). Metoda DEAP osiągnęła minimalnie wyższy wynik CV, ale nie przełożyło się to na lepszy wynik testowy, co można interpretować jako efekt losowości/ograniczonego budżetu ewaluacji lub lekkiego przeoptymalizowania pod CV.

\subsubsection{Zysk jakości: przed vs po optymalizacji}
Końcowym krokiem było zestawienie zmian jakości względem konfiguracji bazowej.

\begin{table}[H]
    \centering
    \caption{Tabela zysków jakości (baseline vs po optymalizacji) dla $F_1$ macro.}
    \label{tab:gains}
    \begin{tabular}{lrrrrrrrr}
        \toprule
        Etap & F1\_CV & F1\_test & Cechy & Metoda & Zysk\_F1\_CV & Zysk\_F1\_test & Zysk\_F1\_CV\_proc & Zysk\_F1\_test\_proc \\
        \midrule
        Przed optymalizacją & 0.9531 & 0.9546 & 9 & Baseline &  &  &  &  \\
        Po optymalizacji & 0.9615 & 0.9632 & 9 & Optuna (Bayesian) & 0.0085 & 0.0086 & 0.89 & 0.91 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/optimization/before_after.png}
    \caption{Wizualizacja porównania jakości przed i po optymalizacji (Cz.3) dla $F_1$ macro.}
    \label{fig:opt_before_after}
\end{figure}

Tabela zysków (Rys. \ref{fig:opt_gains_table}) oraz wykres przed-vs-po (Rys. \ref{fig:opt_before_after}) pokazują, że najlepsza konfiguracja poprawiła wyniki o około $\Delta F_1$\textsubscript{CV}$\approx +0.0085$ (ok. $+0.89\%$) oraz $\Delta F_1$\textsubscript{test}$\approx +0.0086$ (ok. $+0.91\%$) względem baseline. Jest to umiarkowany, ale spójny zysk zarówno na CV, jak i na teście, co sugeruje realną poprawę bez wyraźnych oznak przeuczenia.



\section{Podsumowanie}

Projekt dotyczył budowy i oceny systemu rekomendacji upraw na bazie \textit{Crop Recommendation Dataset}. Ponieważ oryginalny zbiór jest kompletny, brakujące dane zasymulowano w sposób kontrolowany (m.in. w kolumnach \texttt{ph}, \texttt{temperature}, \texttt{humidity}), a następnie porównano strategie imputacji oraz skalowania. Celem była ocena wpływu decyzji preprocessingowych na stabilność i jakość modeli w warunkach bliższych rzeczywistym (z zaszumieniem i brakami danych).

Najważniejsze obserwacje z eksperymentu można podsumować następująco:
\begin{itemize}
    \item \textbf{Imputacja:} proste metody (średnia, mediana) zaniżają wariancję i mogą zniekształcać rozkłady, natomiast podejścia predykcyjne (KNN, Random Forest) lepiej zachowują naturalną dyspersję i relacje pomiędzy cechami.
    \item \textbf{Skalowanie:} standaryzacja i normalizacja skutecznie niwelują różnice skali (np. \texttt{rainfall} vs \texttt{ph}), co jest krytyczne dla modeli opartych o dystans (KNN, SVM) i poprawia stabilność procesu uczenia.
    \item \textbf{Turniej modeli:} po wprowadzeniu "sabotażu" jakości wszystkie modele spadły, jednak metody boostingowe pozostały najbardziej odporne. W praktyce najlepszy wynik uzyskał \textbf{LightGBM}, który został wybrany do dalszej optymalizacji.
    \item \textbf{Selekcja cech (Cz.3):} porównanie SelectKBest (mutual information) i Permutation Importance wykazało, że redukcja do małej liczby cech pogarsza wyniki. Najlepszy rezultat uzyskano dla \textbf{pełnego zestawu 9 cech} (7 bazowych + \texttt{N\_P\_ratio} i \texttt{total\_nutrients}).
    \item \textbf{Tuning hiperparametrów (Cz.3):} porównanie GridSearchCV, Optuna, DEAP oraz AutoML (TPOT) pokazało, że największy zysk jakości uzyskano dzięki \textbf{Optuna}. Względem baseline odnotowano poprawę około $\Delta F_1$\textsubscript{CV}$\approx +0.0085$ oraz $\Delta F_1$\textsubscript{test}$\approx +0.0086$, co oznacza spójny wzrost jakości bez widocznych oznak przeuczenia.
\end{itemize}

Ostatecznie projekt pokazał, że w zadaniu rekomendacji upraw jakość danych wejściowych (imputacja i skalowanie) ma kluczowe znaczenie, a w warunkach zaszumienia najlepszą odporność i potencjał optymalizacji wykazuje LightGBM. Dodatkowe strojenie hiperparametrów przyniosło mierzalne korzyści, natomiast selekcja cech nie poprawiła wyników i w skrajnym wariancie znacząco je pogorszyła.
\bibliographystyle{plain}
\bibliography{bibliography.bib} 

\end{document}